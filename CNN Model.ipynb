{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de583021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import cv2\n",
    "from random import randint\n",
    "import numpy as np\n",
    "CLASSES, gems = [], [] # names of classes, count of images for each class\n",
    "\n",
    "for root, dirs, files in os.walk('C:/Users/Admin/Desktop/DATTA'):\n",
    "    f = os.path.basename(root)    # get class name - a,b,c, etc    \n",
    "        \n",
    "    if len(files) > 0:\n",
    "        gems.append(len(files))\n",
    "        if f not in CLASSES:\n",
    "            CLASSES.append(f) # add folder name\n",
    "    \n",
    "    # uncomment this block if you want a text output about each subfolder\n",
    "    #count_dirs = 0\n",
    "    #for f in dirs:           # count subfolders\n",
    "        #count_dirs += 1\n",
    "    #depth = root.split(os.sep)\n",
    "    #print((len(depth) - 2) * '--'+'>', '{}:\\t {} folders, {} imgs'.format(os.path.basename(root), count_dirs, gems[-1] if gems!=[] else 0)) \n",
    "    \n",
    "gems_count = len(CLASSES) # 27 = number of classes\n",
    "print('{} classes with {} images in total'.format(len(CLASSES), sum(gems)))\n",
    "\n",
    "f, ax = plt.subplots(figsize=(224,224))\n",
    "if(gems[0])<10:\n",
    "    plt.bar(range(gems_count), gems[gems_count:], label = 'train data')\n",
    "    plt.bar(range(gems_count), gems[0:gems_count], label = 'test data')\n",
    "else:\n",
    "    plt.bar(range(gems_count), gems[0:gems_count], label = 'train data')\n",
    "    plt.bar(range(gems_count), gems[gems_count:], label = 'test data')\n",
    "ax.grid()\n",
    "ax.legend(fontsize = 12);\n",
    "\n",
    "img_w, img_h = 48,48  # width and height of image\n",
    "train_dir = 'C:/Users/Admin/Desktop/DATTA/train'\n",
    "\n",
    "def read_imgs_lbls(_dir):\n",
    "    Images, Labels = [], []\n",
    "    for root, dirs, files in os.walk(_dir):\n",
    "        f = os.path.basename(root)  # get class name - Amethyst, Onyx, etc       \n",
    "        for file in files:\n",
    "            Labels.append(f)\n",
    "            try:\n",
    "                image = cv2.imread(root+'/'+file)              # read the image (OpenCV)\n",
    "                image = cv2.resize(image,(int(img_w*1.5), int(img_h*1.5)))       # resize the image (images are different sizes)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # converts an image from BGR color space to RGB\n",
    "                Images.append(image)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    Images = np.array(Images)\n",
    "    return (Images, Labels)\n",
    "def get_class_index(Labels):\n",
    "    for i, n in enumerate(Labels):\n",
    "        for j, k in enumerate(CLASSES):    # foreach CLASSES\n",
    "            if n == k:\n",
    "                Labels[i] = j\n",
    "    Labels = np.array(Labels)\n",
    "    return Labels\n",
    "\n",
    "Train_Imgs, Train_Lbls = read_imgs_lbls(train_dir)\n",
    "Train_Lbls = get_class_index(Train_Lbls)\n",
    "print('Shape of train images: {}'.format(Train_Imgs.shape))\n",
    "print('Shape of train labels: {}'.format(Train_Lbls.shape))#The output you get from your model after training it is called a label\n",
    "\n",
    "dim = 4 #you can change it;  4x4 dimension flat plot\n",
    "\n",
    "f,ax = plt.subplots(dim,dim) \n",
    "f.subplots_adjust(0,0,2,2)\n",
    "for i in range(0,dim):\n",
    "    for j in range(0,dim):\n",
    "        rnd_number = randint(0,len(Train_Imgs))\n",
    "        cl = Train_Lbls[rnd_number]\n",
    "        ax[i,j].imshow(Train_Imgs[rnd_number])\n",
    "        ax[i,j].set_title(CLASSES[cl]+': ' + str(cl))\n",
    "        ax[i,j].axis('off')\n",
    "\n",
    "def edge_and_cut(img):\n",
    "    try:\n",
    "        edges = cv2.Canny(img, img_w, img_h)            \n",
    "        \n",
    "        if(np.count_nonzero(edges)>edges.size/10000):           \n",
    "            pts = np.argwhere(edges>0)\n",
    "            y1,x1 = pts.min(axis=0)\n",
    "            y2,x2 = pts.max(axis=0)\n",
    "            \n",
    "            new_img = img[y1:y2, x1:x2]           # crop the region\n",
    "            new_img = cv2.resize(new_img,(img_w, img_h))  # Convert back\n",
    "        else:\n",
    "            new_img = cv2.resize(img,(img_w, img_h))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        new_img = cv2.resize(img,(img_w, img_h))\n",
    "    return new_img\n",
    "\n",
    "def show_cropped(img):\n",
    "    emb_img = img.copy()\n",
    "    edges = cv2.Canny(img, img_w, img_h)\n",
    "    \n",
    "    if(np.count_nonzero(edges)>edges.size/10000):\n",
    "        pts = np.argwhere(edges>0)\n",
    "        y1,x1 = pts.min(axis=0)\n",
    "        y2,x2 = pts.max(axis=0)\n",
    "        new_img = img[y1:y2, x1:x2]  \n",
    "        edge_size = 1 #replace it with bigger size for larger images            \n",
    "        emb_img[y1-edge_size:y1+edge_size, x1:x2] = [255, 0, 0]\n",
    "        emb_img[y2-edge_size:y2+edge_size, x1:x2] = [255, 0, 0]\n",
    "        emb_img[y1:y2, x1-edge_size:x1+edge_size] = [255, 0, 0]\n",
    "        emb_img[y1:y2, x2-edge_size:x2+edge_size] = [255, 0, 0]\n",
    "\n",
    "        new_img = cv2.resize(new_img,(img_w, img_h))  # Convert to primary size  \n",
    "    else:\n",
    "        new_img = cv2.resize(img,(img_w, img_h))\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(10, 10))\n",
    "    ax[0].imshow(img, cmap='gray')\n",
    "    ax[0].set_title('Original Image', fontsize=14)\n",
    "    ax[1].imshow(edges, cmap='gray')\n",
    "    ax[1].set_title('Canny Edges', fontsize=14)\n",
    "    ax[2].imshow(emb_img, cmap='gray')\n",
    "    ax[2].set_title('Bounding Box', fontsize=14)       \n",
    "    ax[3].imshow(new_img, cmap='gray')\n",
    "    ax[3].set_title('Cropped', fontsize=14)   \n",
    "\n",
    "for x in range(0,3):\n",
    "    show_cropped(Train_Imgs[randint(0,len(Train_Imgs))])\n",
    "\n",
    "def crop_images(Imgs):\n",
    "    CroppedImages = np.ndarray(shape=(len(Imgs), img_w, img_h, 3), dtype=np.int)\n",
    "\n",
    "    ind = 0\n",
    "    for im in Imgs: \n",
    "        x = edge_and_cut(im)\n",
    "        CroppedImages[ind] = x\n",
    "        ind += 1\n",
    "\n",
    "    return CroppedImages\n",
    "\n",
    "Train_Imgs = crop_images(Train_Imgs)\n",
    "print('Final shape of images in train set: {} '.format(Train_Imgs.shape))\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(Train_Imgs, Train_Lbls, shuffle = True, test_size = 0.2, random_state = 42)\n",
    "print('Shape of X_train: {}, y_train: {} '.format(X_train.shape, y_train.shape))\n",
    "print('Shape of X_val: {}, y_val: {} '.format(X_val.shape, y_val.shape))\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "devices = device_lib.list_local_devices()\n",
    "print(devices)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import optimizers\n",
    "\n",
    "filters = 32      # the dimensionality of the output space\n",
    "kernel_size = 3   # length of the 2D convolution window\n",
    "max_pool = 2      # size of the max pooling windows\n",
    "\n",
    "EPOCHS = 10                               # while testing you can change it\n",
    "batch_size = 32                              # number of training samples using in each mini batch during GD (gradient descent) \n",
    "iter_per_epoch = len(X_train) // batch_size  # each sample will be passed [iter_per_epoch] times during training\n",
    "val_per_epoch = len(X_val) // batch_size     # each sample will be passed [val_per_epoch] times during validation\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# first layer\n",
    "model.add(Conv2D(batch_size, (kernel_size, kernel_size), activation='relu', padding='same', input_shape=(img_w, img_h, 3))) # 32\n",
    "model.add(MaxPooling2D((max_pool, max_pool))) #reduce the spatial size of incoming features\n",
    "# second layer\n",
    "model.add(Conv2D(2*batch_size, (kernel_size, kernel_size), activation='relu', padding='same')) # 64\n",
    "model.add(MaxPooling2D((max_pool, max_pool))) \n",
    "# third layer\n",
    "model.add(Conv2D(4*batch_size, (kernel_size, kernel_size), activation='relu', padding='same')) # 128\n",
    "model.add(MaxPooling2D((max_pool, max_pool))) \n",
    "# fourth layer\n",
    "model.add(Conv2D(4*batch_size, (kernel_size, kernel_size), activation='relu', padding='same')) # 128\n",
    "model.add(AveragePooling2D(pool_size= (2, 2), strides= (2, 2))) \n",
    "# fifth layer\n",
    "model.add(Conv2D(4*batch_size, (kernel_size, kernel_size), activation='relu', padding='same')) # 128\n",
    "model.add(MaxPooling2D((max_pool, max_pool)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(16*batch_size, activation='relu'))                                             # 512\n",
    "model.add(Dense(87, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(              # this is the augmentation configuration used for training\n",
    "        rotation_range=25,\n",
    "        zoom_range=0.1,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.2,\n",
    "        horizontal_flip=True\n",
    "        )\n",
    "\n",
    "val_datagen = ImageDataGenerator()                # for val/testing only rescaling function\n",
    "\n",
    "n = randint(0,len(X_train))\n",
    "samples = np.expand_dims(X_train[n], 0)\n",
    "it = train_datagen.flow(samples, batch_size=batch_size)\n",
    "cols = 7\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=cols, figsize=(15, 10))\n",
    "ax[0].imshow(X_train[n], cmap='gray')\n",
    "ax[0].set_title('Original', fontsize=10)\n",
    "\n",
    "for i in range(1,cols):\n",
    "    batch = it.next()    # generate batch of images \n",
    "    image = batch[0].astype('uint32') # convert to unsigned int for viewing\n",
    "    ax[i].set_title('augmented {}'.format(i), fontsize=10)\n",
    "    ax[i].imshow(image, cmap='gray')\n",
    "train_gen = train_datagen.flow(X_train, y_train, batch_size=batch_size)\n",
    "val_gen = val_datagen.flow(X_val, y_val, batch_size=batch_size)\n",
    "\n",
    "m = model.fit_generator(\n",
    "       train_gen,\n",
    "       steps_per_epoch= iter_per_epoch,\n",
    "       epochs=EPOCHS, \n",
    "       validation_data = val_gen,\n",
    "       validation_steps = val_per_epoch,\n",
    "       verbose = 1 # Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
    "       )\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\n",
    "axs[0].plot(m.history['accuracy'])\n",
    "axs[0].plot(m.history['val_accuracy'])\n",
    "axs[0].set_title('Model accuracy')\n",
    "axs[0].legend(['Train', 'Val'], loc='upper left')\n",
    "\n",
    "axs[1].plot(m.history['loss'])\n",
    "axs[1].plot(m.history['val_loss'])\n",
    "axs[1].set_title('Model loss')\n",
    "axs[1].legend(['Train', 'Val'], loc='upper left')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel='Epoch')\n",
    "\n",
    "score = model.evaluate_generator(val_gen, steps= len(val_gen))\n",
    "\n",
    "for idx, metric in enumerate(model.metrics_names):\n",
    "    print('{}:{}'.format(metric, score[idx]))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pre_test=model.predict(X_val)\n",
    "y_pre_test=np.argmax(y_pre_test,axis=1)\n",
    "cm=confusion_matrix(y_val,y_pre_test)\n",
    "\n",
    "plt.figure(figsize = (15,15))\n",
    "sn.heatmap(cm, annot=True)\n",
    "\n",
    "x=(y_pre_test-y_val!=0).tolist()\n",
    "x=[i for i,l in enumerate(x) if l!=False]\n",
    "\n",
    "fig,ax=plt.subplots(1,5,sharey=False,figsize=(13,13))\n",
    "fig.tight_layout()\n",
    "\n",
    "for i in range(5):\n",
    "    ax[i].imshow(X_val[x[i]][:,:,1])\n",
    "    ax[i].set_xlabel('{}, Pred: {}'.format(CLASSES[y_val[x[i]]],CLASSES[y_pre_test[x[i]]]))\n",
    "\n",
    "hand_gesture = ['unknown', 'd', 'c', 'z', 'a']\n",
    "hand_gesture = get_class_index(hand_gesture)\n",
    "\n",
    "fig,ax=plt.subplots(1,len(hand_gesture),sharey=False,figsize=(13,13))\n",
    "fig.tight_layout()\n",
    "\n",
    "for i in range(len(hand_gesture)):\n",
    "    ax[i].imshow(Train_Imgs[np.where(Train_Lbls==hand_gesture[i])[0][1]])\n",
    "    ax[i].set_xlabel(CLASSES[hand_gesture[i]])\n",
    "\n",
    "model.save('cnn.h5')\n",
    "\n",
    "from keras.models import load_model\n",
    "new_model=load_model('cnn.h5')\n",
    "test_dir = 'C:/Users/Admi/Desktop/DATTA/test'\n",
    "Test_Imgs, Test_Lbls = read_imgs_lbls(test_dir)\n",
    "Test_Lbls = get_class_index(Test_Lbls)\n",
    "Test_Imgs = crop_images(Test_Imgs)\n",
    "print('shape of images in test set: {} '.format(Test_Imgs.shape))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
